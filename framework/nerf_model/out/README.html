<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>NeRF-pytorch</title>
        <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
        
    </head>
    <body class="vscode-body vscode-light">
        <h1 id="nerf-pytorch">NeRF-pytorch</h1>
<p><a href="http://www.matthewtancik.com/nerf">NeRF</a> (Neural Radiance Fields) is a method that achieves state-of-the-art results for synthesizing novel views of complex scenes. Here are some videos generated by this repository (pre-trained models are provided below):</p>
<p><img src="https://user-images.githubusercontent.com/7057863/78472232-cf374a00-7769-11ea-8871-0bc710951839.gif" alt="">
<img src="https://user-images.githubusercontent.com/7057863/78472235-d1010d80-7769-11ea-9be9-51365180e063.gif" alt=""></p>
<p>This project is a faithful PyTorch implementation of <a href="http://www.matthewtancik.com/nerf">NeRF</a> that <strong>reproduces</strong> the results while running <strong>1.3 times faster</strong>. The code is based on authors' Tensorflow implementation <a href="https://github.com/bmild/nerf">here</a>, and has been tested to match it numerically.</p>
<h2 id="installation">Installation</h2>
<pre><code>git clone https://github.com/yenchenlin/nerf-pytorch.git
cd nerf-pytorch
pip install -r requirements.txt
</code></pre>
<details>
  <summary> Dependencies (click to expand) </summary>
<h2 id="dependencies">Dependencies</h2>
<ul>
<li>PyTorch 1.4</li>
<li>matplotlib</li>
<li>numpy</li>
<li>imageio</li>
<li>imageio-ffmpeg</li>
<li>configargparse</li>
</ul>
<p>The LLFF data loader requires ImageMagick.</p>
<p>You will also need the <a href="http://github.com/fyusion/llff">LLFF code</a> (and COLMAP) set up to compute poses if you want to run on your own real data.</p>
</details>
<h2 id="how-to-run">How To Run?</h2>
<h3 id="quick-start">Quick Start</h3>
<p>Download data for two example datasets: <code>lego</code> and <code>fern</code></p>
<pre><code>bash download_example_data.sh
</code></pre>
<p>To train a low-res <code>lego</code> NeRF:</p>
<pre><code>python run_nerf.py --config configs/lego.txt
</code></pre>
<p>After training for 100k iterations (~4 hours on a single 2080 Ti), you can find the following video at <code>logs/lego_test/lego_test_spiral_100000_rgb.mp4</code>.</p>
<p><img src="https://user-images.githubusercontent.com/7057863/78473103-9353b300-7770-11ea-98ed-6ba2d877b62c.gif" alt=""></p>
<hr>
<p>To train a low-res <code>fern</code> NeRF:</p>
<pre><code>python run_nerf.py --config configs/fern.txt
</code></pre>
<p>After training for 200k iterations (~8 hours on a single 2080 Ti), you can find the following video at <code>logs/fern_test/fern_test_spiral_200000_rgb.mp4</code> and <code>logs/fern_test/fern_test_spiral_200000_disp.mp4</code></p>
<p><img src="https://user-images.githubusercontent.com/7057863/78473081-58ea1600-7770-11ea-92ce-2bbf6a3f9add.gif" alt=""></p>
<hr>
<h3 id="more-datasets">More Datasets</h3>
<p>To play with other scenes presented in the paper, download the data <a href="https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1">here</a>. Place the downloaded dataset according to the following directory structure:</p>
<pre><code>├── configs                                                                                                       
│   ├── ...                                                                                     
│                                                                                               
├── data                                                                                                                                                                                                       
│   ├── nerf_llff_data                                                                                                  
│   │   └── fern                                                                                                                             
│   │   └── flower  # downloaded llff dataset                                                                                  
│   │   └── horns   # downloaded llff dataset
|   |   └── ...
|   ├── nerf_synthetic
|   |   └── lego
|   |   └── ship    # downloaded synthetic dataset
|   |   └── ...
</code></pre>
<hr>
<p>To train NeRF on different datasets:</p>
<pre><code>python run_nerf.py --config configs/{DATASET}.txt
</code></pre>
<p>replace <code>{DATASET}</code> with <code>trex</code> | <code>horns</code> | <code>flower</code> | <code>fortress</code> | <code>lego</code> | etc.</p>
<hr>
<p>To test NeRF trained on different datasets:</p>
<pre><code>python run_nerf.py --config configs/{DATASET}.txt --render_only
</code></pre>
<p>replace <code>{DATASET}</code> with <code>trex</code> | <code>horns</code> | <code>flower</code> | <code>fortress</code> | <code>lego</code> | etc.</p>
<h3 id="pre-trained-models">Pre-trained Models</h3>
<p>You can download the pre-trained models <a href="https://drive.google.com/drive/folders/1jIr8dkvefrQmv737fFm2isiT6tqpbTbv">here</a>. Place the downloaded directory in <code>./logs</code> in order to test it later. See the following directory structure for an example:</p>
<pre><code>├── logs 
│   ├── fern_test
│   ├── flower_test  # downloaded logs
│   ├── trex_test    # downloaded logs
</code></pre>
<h3 id="reproducibility">Reproducibility</h3>
<p>Tests that ensure the results of all functions and training loop match the official implentation are contained in a different branch <code>reproduce</code>. One can check it out and run the tests:</p>
<pre><code>git checkout reproduce
py.test
</code></pre>
<h2 id="method">Method</h2>
<p><a href="http://tancik.com/nerf">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a><br>
<a href="https://people.eecs.berkeley.edu/~bmild/">Ben Mildenhall</a>*<sup>1</sup>,
<a href="https://people.eecs.berkeley.edu/~pratul/">Pratul P. Srinivasan</a>*<sup>1</sup>,
<a href="http://tancik.com/">Matthew Tancik</a>*<sup>1</sup>,
<a href="http://jonbarron.info/">Jonathan T. Barron</a><sup>2</sup>,
<a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a><sup>3</sup>,
<a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a><sup>1</sup> <br>
<sup>1</sup>UC Berkeley, <sup>2</sup>Google Research, <sup>3</sup>UC San Diego<br>
*denotes equal contribution</p>
<img src='imgs/pipeline.jpg'/>
<blockquote>
<p>A neural radiance field is a simple fully connected network (weights are ~5MB) trained to reproduce input views of a single scene using a rendering loss. The network directly maps from spatial location and viewing direction (5D input) to color and opacity (4D output), acting as the &quot;volume&quot; so we can use volume rendering to differentiably render new views</p>
</blockquote>
<h2 id="citation">Citation</h2>
<p>Kudos to the authors for their amazing results:</p>
<pre><code>@misc{mildenhall2020nerf,
    title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
    author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
    year={2020},
    eprint={2003.08934},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre>
<p>However, if you find this implementation or pre-trained models helpful, please consider to cite:</p>
<pre><code>@misc{lin2020nerfpytorch,
  title={NeRF-pytorch},
  author={Yen-Chen, Lin},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished={\url{https://github.com/yenchenlin/nerf-pytorch/}},
  year={2020}
}
</code></pre>

        
        
    </body>
    </html>